{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps Demo\n",
    "\n",
    "Julian Bright, Machine Learning Specialist @ Amazon Web Services\n",
    "\n",
    "\n",
    "### Overview\n",
    "\n",
    "Following a series of steps to trigger demo\n",
    "\n",
    "1. [Data Prep](#Data-Prep)\n",
    "2. [Start Build](#Start-Build)\n",
    "3. [Waiting for Training Job](#Wait-for-Training-Job)\n",
    "4. [Test Dev Deployment](#Test-Dev-Deployment)\n",
    "5. [Test Prod Endpoint](#Test-Prod-Endpoint)\n",
    "6. [Monitor](#Monitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep\n",
    "\n",
    "Download a sample of the New York City Taxi [dataset](https://registry.opendata.aws/nyc-tlc-trip-records-pds/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp 's3://nyc-tlc/trip data/green_tripdata_2018-02.csv' 'nyc-tlc.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset into a pandas data frame (this should take approximately 20 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "\n",
    "parse_dates= ['lpep_dropoff_datetime', 'lpep_pickup_datetime']\n",
    "trip_df = pd.read_csv('nyc-tlc.csv', parse_dates=parse_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering to convert dates and add derived duration in minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_df['duration_minutes'] = (trip_df['lpep_dropoff_datetime'] - trip_df['lpep_pickup_datetime']).dt.seconds/60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a sample of columns for our machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['total_amount', 'duration_minutes', 'passenger_count', 'trip_distance']\n",
    "data_df = trip_df[cols]\n",
    "print(data_df.shape)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude any outlines, dropping any null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df[(data_df.total_amount > 0) & (data_df.total_amount < 200) & \n",
    "                  (data_df.duration_minutes > 0) & (data_df.duration_minutes < 120) & \n",
    "                  (data_df.trip_distance > 0) & (data_df.trip_distance < 1000) & \n",
    "                  (data_df.passenger_count > 0)].dropna()\n",
    "print(data_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize \n",
    "\n",
    "Sample and plot distribution of fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = data_df.sample(1000)\n",
    "sample_df.hist(bins=100, layout=(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot duration vs trip distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.plot.scatter(x='duration_minutes', y='trip_distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot duration vs total amount and we see a similar patttern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.plot.scatter(x='duration_minutes', y='total_amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to split the dataset into train/validation/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(data_df, test_size=0.20, random_state=42)\n",
    "val_df, test_df = train_test_split(val_df, test_size=0.05, random_state=42)\n",
    "\n",
    "# Set the index for our test dataframe\n",
    "test_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print('split train: {}, val: {}, test: {} '.format(train_df.shape[0], val_df.shape[0], test_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save files as CSV including baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_cols = ['total_amount', 'duration_minutes','passenger_count','trip_distance']\n",
    "train_df.to_csv('train.csv', index=False, header=False)\n",
    "val_df.to_csv('validation.csv', index=False, header=False)\n",
    "\n",
    "# Save test and baseline with headers\n",
    "test_df.to_csv('test.csv', index=False, header=True)\n",
    "train_df.to_csv('baseline.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload files to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "# Get the session and default bucket\n",
    "session = sagemaker.session.Session()\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "# Specify data previx version\n",
    "prefix = 'nyc-tlc/v1'\n",
    "\n",
    "s3_train_uri = session.upload_data('train.csv', bucket, prefix + '/data/training')\n",
    "s3_val_uri = session.upload_data('validation.csv', bucket, prefix + '/data/validation')\n",
    "s3_baseline_uri = session.upload_data('baseline.csv', bucket, prefix + '/data/baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Build\n",
    "\n",
    "Load variables from environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import time\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "artifact_bucket = os.environ['ARTIFACT_BUCKET']\n",
    "pipeline_name = os.environ['PIPELINE_NAME']\n",
    "model_name = os.environ['MODEL_NAME']\n",
    "\n",
    "print('region: {}'.format(region))\n",
    "print('artifact bucket: {}'.format(artifact_bucket))\n",
    "print('pipeline: {}'.format(pipeline_name))\n",
    "print('model name: {}'.format(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload data source meta data to trigger a new build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import zipfile\n",
    "import json\n",
    "\n",
    "input_data = {\n",
    "    'TrainingUri': s3_train_uri,\n",
    "    'ValidationUri': s3_val_uri,\n",
    "    'BaselineUri': s3_baseline_uri\n",
    "}\n",
    "\n",
    "hyperparameters = {\n",
    "    'num_round': 50\n",
    "}\n",
    "\n",
    "data_source_key = '{}/data-source.zip'.format(pipeline_name)\n",
    "\n",
    "zip_buffer = BytesIO()\n",
    "with zipfile.ZipFile(zip_buffer, 'a') as zf:\n",
    "    zf.writestr('inputData.json', json.dumps(input_data))\n",
    "    zf.writestr('hyperparameters.json', json.dumps(hyperparameters))\n",
    "zip_buffer.seek(0)\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "s3.put_object(Bucket=artifact_bucket, Key=data_source_key, Body=bytearray(zip_buffer.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait for Training Job\n",
    "\n",
    "Follow the code pipeline to wait until the training job is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML('<a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/codesuite/codepipeline/pipelines/{1}/view?region={0}\">Code Pipeline</a>'.format(region, pipeline_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Dev Deployment\n",
    "\n",
    "One the endpoint has been deployed and awaiting approval, we can begin some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codepipeline = boto3.client('codepipeline')\n",
    "\n",
    "def get_pipeline_stage(pipeline_name, stage_name):\n",
    "    response = codepipeline.get_pipeline_state(name=pipeline_name)\n",
    "    for stage in response['stageStates']:\n",
    "        if stage['stageName'] == stage_name:\n",
    "            return stage\n",
    "        \n",
    "# Get last execution id\n",
    "deploy_dev = get_pipeline_stage(pipeline_name, 'DeployDev')\n",
    "if not 'latestExecution' in deploy_dev:\n",
    "    raise(Exception('Please wait.  Deploy dev not complete'))\n",
    "    \n",
    "execution_id = deploy_dev['latestExecution']['pipelineExecutionId']\n",
    "dev_endpoint_name = 'mlops-{}-dev-{}'.format(model_name, execution_id)\n",
    "\n",
    "print('endpoint name: {}'.format(dev_endpoint_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait until the dev endpoint is in service (this can take up to 10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = boto3.client('sagemaker')\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        response = sm.describe_endpoint(EndpointName=dev_endpoint_name)\n",
    "        print(\"Endpoint status: {}\".format(response['EndpointStatus']))\n",
    "        if response['EndpointStatus'] == 'InService':\n",
    "            break\n",
    "    except:\n",
    "        pass \n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import RealTimePredictor, csv_serializer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_predictor(endpoint_name):\n",
    "    xgb_predictor = RealTimePredictor(endpoint_name)\n",
    "    xgb_predictor.content_type = 'text/csv'\n",
    "    xgb_predictor.serializer = csv_serializer\n",
    "    return xgb_predictor\n",
    "\n",
    "def predict(predictor, data, rows=500):\n",
    "    split_array = np.array_split(data, round(data.shape[0] / float(rows)))\n",
    "    predictions = ''\n",
    "    for array in tqdm(split_array):\n",
    "        predictions = ','.join([predictions, predictor.predict(array).decode('utf-8')])\n",
    "    return np.fromstring(predictions[1:], sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke the dev endpoint with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_predictor = get_predictor(dev_endpoint_name)\n",
    "predictions = predict(dev_predictor, test_df[test_df.columns[1:]].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the response into a dataframe, and join with predictions to calculate absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({'total_amount_predictions': predictions })\n",
    "pred_df = test_df.join(pred_df) # Join on all\n",
    "pred_df['error'] = abs(pred_df['total_amount']-pred_df['total_amount_predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some of the largest errors are high amounts for low distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.sort_values('error', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the total amount vs predicterd for the test error inspecting some outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pred_df.tail(1000).plot.scatter(x='total_amount_predictions', y='total_amount', \n",
    "                                     c='error', title='actual amount vs pred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the root means square error for the predicted total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def rmse(pred_df):\n",
    "    return sqrt(mean_squared_error(pred_df['total_amount'], pred_df['total_amount_predictions']))\n",
    "\n",
    "print('RMSE: {}'.format(rmse(pred_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are happy with this metric, we can go ahead and approve this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "def on_click(obj):\n",
    "    result = { 'summary': approval_text.value, 'status': obj.description }\n",
    "    response = codepipeline.put_approval_result(\n",
    "      pipelineName=pipeline_name,\n",
    "      stageName='DeployDev',\n",
    "      actionName='ApproveDeploy',\n",
    "      result=result,\n",
    "      token=approval_action['token']\n",
    "    )\n",
    "    button_box.close()\n",
    "    print(result)\n",
    "\n",
    "# Create the widget if we are ready for approval\n",
    "approval_action = get_pipeline_stage(pipeline_name, 'DeployDev')['actionStates'][-1]['latestExecution']\n",
    "if 'token' in approval_action:\n",
    "    approval_text = widgets.Text(placeholder='Optional approval message')   \n",
    "    approve_btn = widgets.Button(description=\"Approved\", button_style='success', icon='check')\n",
    "    reject_btn = widgets.Button(description=\"Rejected\", button_style='danger', icon='close')\n",
    "    approve_btn.on_click(on_click)\n",
    "    reject_btn.on_click(on_click)\n",
    "    button_box = widgets.HBox([approval_text, approve_btn, reject_btn])\n",
    "    display(button_box)\n",
    "else:\n",
    "    raise(Exception('Please wait.  No dev approval'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Prod Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the current state of the production deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "stack_name = stack_name='{}-deploy-prd'.format(pipeline_name)\n",
    "print('stack name: {}'.format(stack_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the last events and how long ago they occured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.tz import tzlocal\n",
    "\n",
    "def get_event_dataframe(events):\n",
    "    stack_cols = ['LogicalResourceId', 'ResourceStatus', 'ResourceStatusReason', 'Timestamp']\n",
    "    stack_event_df = pd.DataFrame(events)[stack_cols].fillna('')\n",
    "    stack_event_df['TimeAgo'] = (datetime.now(tzlocal())-stack_event_df['Timestamp'])\n",
    "    return stack_event_df.drop('Timestamp', axis=1)\n",
    "\n",
    "# Get latest stack events\n",
    "response = cfn.describe_stack_events(StackName=stack_name)\n",
    "get_event_dataframe(response['StackEvents']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Prod Endpoint\n",
    "\n",
    "We can send some traffic to the production endpoint now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_endpoint_name='mlops-{}-prd-{}'.format(model_name, execution_id)\n",
    "print('prod endpoint: {}'.format(prd_endpoint_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait until the endpoint has finishing updated before we send some traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = boto3.client('sagemaker')\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        response = sm.describe_endpoint(EndpointName=prd_endpoint_name)\n",
    "        print(\"Endpoint status: {}\".format(response['EndpointStatus']))\n",
    "        # Wait until the endpoint is in service with data capture enabled\n",
    "        if response['EndpointStatus'] == 'InService' \\\n",
    "            and 'DataCaptureConfig' in response \\\n",
    "            and response['DataCaptureConfig']['EnableCapture']:\n",
    "            break\n",
    "    except:\n",
    "        pass\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send some inference to production endpoint now that data capture is enabled.  Use single records to that monitoring schedule can map to baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_predictor = get_predictor(prd_endpoint_name)\n",
    "sample_values = test_df[test_df.columns[1:]].sample(100).values\n",
    "predictions = predict(prd_predictor, sample_values, rows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Rest API\n",
    "\n",
    "Get back the deployment progress and rest api endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stack_status(stack_name):\n",
    "    response = cfn.describe_stacks(StackName=stack_name)\n",
    "    if response['Stacks']:\n",
    "        stack = response['Stacks'][0]\n",
    "        return stack['StackStatus'], dict([(o['OutputKey'], o['OutputValue']) for o in stack['Outputs']])\n",
    "\n",
    "status, outputs = get_stack_status(stack_name)\n",
    "            \n",
    "print('stack status: {}'.format(status))\n",
    "print('deployment application: {}'.format(outputs['DeploymentApplication']))\n",
    "print('rest api: {}'.format(outputs['RestApi']))                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the deployment application to see if its created and started to shift traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HTML('<a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/codesuite/codedeploy/applications/{1}?region={0}\">Deployment Application</a>'.format(region, outputs['DeploymentApplication']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's ping the REST endpoint to see which sagemaker endpoint it is hitting.  Press STOP when deployment complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from urllib import request\n",
    "\n",
    "headers = {\"Content-type\": \"text/csv\"}\n",
    "payload = test_df[test_df.columns[1:]].head(1).to_csv(header=False, index=False).encode('utf-8')\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        resp = request.urlopen(request.Request(outputs['RestApi'], data=payload, headers=headers))\n",
    "        print(\"Response code: %d: endpoint: %s\" % (resp.getcode(), resp.getheader('x-sagemaker-endpoint')))\n",
    "        status, outputs = get_stack_status(stack_name) \n",
    "        if status.endswith('COMPLETE'):\n",
    "            print('Deployment complete\\n')\n",
    "            break\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor\n",
    "\n",
    "Get the latest production deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get last execution id\n",
    "deploy_prd = get_pipeline_stage(pipeline_name, 'DeployPrd')\n",
    "if not 'latestExecution' in deploy_prd:\n",
    "    raise(Exception('Please wait.  Prod prd not complete'))\n",
    "    \n",
    "execution_id = deploy_prd['latestExecution']['pipelineExecutionId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "Load baseline processing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_job_name='mlops-{}-pbl-{}'.format(model_name, execution_id)\n",
    "schedule_name='mlops-{}-pms-{}'.format(model_name, execution_id)\n",
    "\n",
    "print('processing job name: {}'.format(processing_job_name))\n",
    "print('schedule name: {}'.format(schedule_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.model_monitor import BaseliningJob, DefaultModelMonitor, MonitoringExecution\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "baseline_job = BaseliningJob.from_processing_name(sagemaker_session, processing_job_name)\n",
    "status = baseline_job.describe()['ProcessingJobStatus']\n",
    "if status != 'Completed':\n",
    "    raise(Exception('Please wait. Processing job not complete, status: {}'.format(status)))\n",
    "    \n",
    "baseline_results_uri  = baseline_job.outputs[0].destination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the generated constraints and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "baseline_statistics = baseline_job.baseline_statistics().body_dict\n",
    "schema_df = pd.io.json.json_normalize(baseline_statistics[\"features\"])\n",
    "schema_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_constraints = baseline_job.suggested_constraints().body_dict\n",
    "constraints_df = pd.io.json.json_normalize(baseline_constraints[\"features\"])\n",
    "constraints_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data Capture\n",
    "\n",
    "Get the list of data capture files form the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = sagemaker_session.default_bucket()\n",
    "data_capture_logs_uri = 's3://{}/{}/datacapture/{}'.format(bucket, model_name, prd_endpoint_name)\n",
    "\n",
    "capture_files = S3Downloader.list(data_capture_logs_uri)\n",
    "print('Found {} files'.format(len(capture_files)))\n",
    "\n",
    "if capture_files:\n",
    "    # Get the first line of the most recent file    \n",
    "    event = json.loads(S3Downloader.read_file(capture_files[-1]).split('\\n')[0])\n",
    "    print('\\nLast file:\\n{}'.format(json.dumps(event, indent=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Monitoring Schedule\n",
    "\n",
    "The functions for plotting and rendering distribution statistics or constraint violations are implemented in a `utils` file so let's grab that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O utils.py --quiet https://raw.githubusercontent.com/awslabs/amazon-sagemaker-examples/master/sagemaker_model_monitor/visualization/utils.py\n",
    "import utils as mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the last succesful monitoring schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that we are looking for completed/stopped schedules\n",
    "sm = boto3.client('sagemaker')\n",
    "response = sm.list_monitoring_executions(MonitoringScheduleName=schedule_name)\n",
    "\n",
    "status = None\n",
    "expected_status = ['Completed', 'CompletedWithViolations']\n",
    "for mon in response['MonitoringExecutionSummaries']:\n",
    "    processing_job_arn = mon['ProcessingJobArn']\n",
    "    status = mon['MonitoringExecutionStatus']\n",
    "    if status in expected_status:\n",
    "        break\n",
    "\n",
    "if not status in expected_status:\n",
    "    raise(Exception('Please wait.  No completed schedules'))\n",
    "    \n",
    "print('Schedule status: {}'.format(status))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the monitoring execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = MonitoringExecution.from_processing_arn(sagemaker_session=sagemaker.Session(), \n",
    "                                                    processing_job_arn=processing_job_arn)\n",
    "exec_inputs = {inp['InputName']: inp for inp in execution.describe()['ProcessingInputs']}\n",
    "exec_results_uri = execution.output.destination\n",
    "\n",
    "print('Monitoring Execution results: {}'.format(exec_results_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the constraints, statistics and violations if they exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls $exec_results_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the baseline and monitoring statistics & violations\n",
    "baseline_statistics = baseline_job.baseline_statistics().body_dict\n",
    "execution_statistics = execution.statistics().body_dict\n",
    "violations = execution.constraint_violations().body_dict['violations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mu.show_violation_df(baseline_statistics=baseline_statistics, \n",
    "                     latest_statistics=execution_statistics, \n",
    "                     violations=violations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
